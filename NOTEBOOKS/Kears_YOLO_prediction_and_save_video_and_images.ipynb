{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pediction of images and video using Keras YOLO v2 models\n",
    "\n",
    "This notebook detects objects in images and videos using trained Keras YOLO v2 models. This notebook should be run from inside the [root folder](https://github.com/experiencor/keras-yolo2) of the Keras YOLO git project. Also, the respective back-end, configuration file and trained model should also be placed in the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from preprocessing import parse_annotation\n",
    "from utils import draw_boxes\n",
    "from frontend import YOLO\n",
    "import json\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instructions from: https://michaelblogscode.wordpress.com/2017/10/10/reducing-and-profiling-gpu-memory-usage-in-keras-with-tensorflow-backend/\n",
    "\n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "tf.keras.backend.set_session(tf.Session(config=config))\n",
    "#keras.backend.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set/adjust the parameters required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = './config_full_yolo.json'\n",
    "weights_path = './full_yolo_singapore_dataset.h5'\n",
    "input_folder = '/home/tbontz2s/git/tensorflow/workspace/training_demo/images/test'\n",
    "output_folder = '/home/tbontz2s/RESULTS_PHOTOS/yolo_results/full_yolo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 13)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 416, 416, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 (None, 13, 13, 1024) 50547936    input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "DetectionLayer (Conv2D)         (None, 13, 13, 70)   71750       model_3[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 13, 13, 5, 14 0           DetectionLayer[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1, 1, 1, 10,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 13, 13, 5, 14 0           reshape_2[0][0]                  \n",
      "                                                                 input_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 50,619,686\n",
      "Trainable params: 50,599,014\n",
      "Non-trainable params: 20,672\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with open(config_path) as config_buffer:    \n",
    "    config = json.load(config_buffer)\n",
    "\n",
    "###############################\n",
    "#   Make the model \n",
    "###############################\n",
    "\n",
    "yolo = YOLO(backend             = config['model']['backend'],\n",
    "            input_size          = config['model']['input_size'], \n",
    "            labels              = config['model']['labels'], \n",
    "            max_box_per_image   = config['model']['max_box_per_image'],\n",
    "            anchors             = config['model']['anchors'])\n",
    "\n",
    "###############################\n",
    "#   Load trained weights\n",
    "###############################    \n",
    "\n",
    "yolo.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all the images to be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select some handpicked images\n",
    "images_filenames = [\n",
    "    'MVI_1468_NIR_frame245.jpg',\n",
    "    'MVI_1469_VIS_frame470.jpg',\n",
    "    'MVI_1474_VIS_frame425.jpg',\n",
    "    'MVI_1486_VIS_frame620.jpg',\n",
    "    'MVI_1578_VIS_frame490.jpg',\n",
    "    'MVI_1609_VIS_frame400.jpg',\n",
    "    'MVI_0797_VIS_OB_frame425.jpg',\n",
    "    'MVI_1520_NIR_frame490.jpg',\n",
    "    'MVI_0895_NIR_Haze_frame340.jpg'\n",
    "]\n",
    "\n",
    "images = [os.path.join(input_folder, image_filename) for image_filename in images_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 boxes are found\n",
      "6 boxes are found\n",
      "14 boxes are found\n",
      "4 boxes are found\n",
      "7 boxes are found\n",
      "7 boxes are found\n",
      "3 boxes are found\n",
      "2 boxes are found\n",
      "2 boxes are found\n"
     ]
    }
   ],
   "source": [
    "for i, img in enumerate(images):\n",
    "    image = cv2.imread(img)\n",
    "    boxes = yolo.predict(image)\n",
    "    image = draw_boxes(image, boxes, config['model']['labels'])\n",
    "    \n",
    "    print(len(boxes), 'boxes are found')    \n",
    "\n",
    "    cv2.imwrite(os.path.join(output_folder, img.split('/')[-1]), image)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict an unlabelled video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:22<00:00, 11.47it/s]\n"
     ]
    }
   ],
   "source": [
    "image_path = '/home/tbontz2s/singapore_dataset/VIS_Onshore/Videos/MVI_1470_VIS.avi'\n",
    "\n",
    "video_out = image_path[:-4] + '_detected' + image_path[-4:]\n",
    "video_reader = cv2.VideoCapture(image_path)\n",
    "\n",
    "nb_frames = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_h = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_w = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "video_writer = cv2.VideoWriter(video_out,\n",
    "                       cv2.VideoWriter_fourcc(*'MPEG'), \n",
    "                       50.0, \n",
    "                       (frame_w, frame_h))\n",
    "\n",
    "for i in tqdm(range(nb_frames)):\n",
    "    _, image = video_reader.read()\n",
    "\n",
    "    boxes = yolo.predict(image)\n",
    "    image = draw_boxes(image, boxes, config['model']['labels'])\n",
    "\n",
    "    video_writer.write(np.uint8(image))\n",
    "\n",
    "video_reader.release()\n",
    "video_writer.release() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
